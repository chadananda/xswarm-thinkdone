# ThinkDone Meeting Dynamics
## AI Behavioral Specification

**Document type:** AI behavior spec for the meeting engine
**Audience:** Claude Code building the conversation/agenda/extraction logic
**Scope:** How the AI thinks, leads, asks, extracts, and adapts â€” NOT how the dashboard looks
**Reference:** thinkdone-soul-concept.md (data structures, memory schema, engines)

---

## 1. What This Document Covers

This spec defines how the AI runs meetings: generating agendas from data, leading structured conversations, extracting actionable items, managing agenda state as the conversation progresses, and adapting its behavior based on user patterns. The dashboard already exists. This is the brain behind it.

---

## 2. Meeting Types

Five meeting types. Each has a distinct AI posture, agenda generation strategy, and conversational cadence.

### 2.1 Morning Meeting (PLANNING SESSION)

**When:** First app open of the day, after overnight batch has run.
**AI posture:** Confident, prepared, leading. Has done all homework overnight.
**Agenda source:** Pre-generated by sleep-time engine + thinking queue results + carryover from previous meetings.
**Typical agenda size:** 6â€“12 items.
**Target duration:** 5â€“15 minutes, 6â€“15 conversational turns.
**Model routing:** First turn Sonnet (delivery quality matters for the opening), follow-ups routed by complexity â€” Haiku for acknowledgments and simple extractions, Sonnet for synthesis and strategic suggestions.

### 2.2 Ad-Hoc Check-In

**When:** User opens conversation after morning meeting has already occurred today.
**AI posture:** Efficient, context-aware, responsive. The user is driving.
**Agenda source:** No pre-generated agenda. The AI builds context dynamically from the user's opening message + current state (what was planned this morning, what's been completed, what's overdue).
**Typical agenda size:** 0â€“3 items (often zero â€” user has a specific need).
**Target duration:** 1â€“5 minutes, 1â€“5 turns.
**Model routing:** Haiku for simple status updates ("mark Arabic done"), Sonnet if the user opens a complex topic.

### 2.3 Evening Review

**When:** After 7 PM (configurable), if user opens app and hasn't done an evening review today. Can also be triggered explicitly by user.
**AI posture:** Reflective, warm, celebratory of wins, gentle about misses. Wind-down energy.
**Agenda source:** Auto-generated by comparing morning plan to actual completions, plus any unresolved items from the day.
**Typical agenda size:** 3â€“6 items.
**Target duration:** 3â€“8 minutes, 4â€“8 turns.
**Model routing:** Sonnet for the opening reflection, Haiku for follow-ups.

### 2.4 Weekly Review

**When:** Once per week at user's preferred day/time (configurable). Replaces the morning meeting that day.
**AI posture:** Thorough, methodical, strategic. The deepest structured interaction.
**Agenda source:** Pre-generated by overnight batch with extensive week analysis. Follows a 6-phase GTD review structure.
**Typical agenda size:** 12â€“25 items across multiple phases.
**Target duration:** 15â€“30 minutes, 15â€“30 turns.
**Model routing:** Sonnet throughout (this interaction justifies the cost).

### 2.5 Strategic Session

**When:** User explicitly initiates a deep dive ("Let's think about the DRBI grant strategy").
**AI posture:** Collaborative, Socratic, probing. Draws heavily on memory and research.
**Agenda source:** Dynamically generated from the user's topic + all relevant memories/context. Agenda items emerge during conversation rather than being pre-set.
**Typical agenda size:** 3â€“8 items, highly dynamic.
**Target duration:** 10â€“30 minutes, 10â€“25 turns.
**Model routing:** Sonnet (complex reasoning required throughout).

---

## 3. Agenda Generation

The agenda is the backbone of every meeting. It's a priority queue of items the AI works through, built from multiple data sources.

### 3.1 Agenda Item Structure

Each agenda item contains:

- **id** â€” unique within this meeting session
- **type** â€” what kind of interaction this item requires (see Â§3.3)
- **priority** â€” critical / high / normal / low
- **content** â€” what this item is about (internal reference for the AI)
- **question** â€” the question or prompt to present to the user (null for inform-only items)
- **followUps** â€” pre-loaded follow-up questions the AI can use if the user's response is vague
- **context** â€” compressed context the AI needs (relevant memories, calendar data, etc.)
- **sourceMemoryIds** â€” which memories this item was generated from (for linking decisions back to source data)
- **status** â€” pending / active / resolved / deferred / spawned
- **resolution** â€” what was decided or captured when the item was resolved
- **spawnedItems** â€” IDs of new items this one generated during conversation
- **estimatedTurns** â€” how many conversation turns this item likely needs (used for time budgeting)

### 3.2 Agenda Item Lifecycle

```
pending â†’ active â†’ resolved     (normal: discussed and completed)
pending â†’ active â†’ deferred     (user says "not now" / "later" / "skip")
pending â†’ active â†’ spawned      (generates sub-items; parent resolves when children do)
pending â†’ skipped               (AI determines item is no longer relevant mid-meeting)
```

Deferred items carry over to the next meeting with their priority escalated by one level (normal â†’ high, high â†’ critical). Items deferred 3+ times get flagged â€” the AI should change its approach rather than keep presenting the same item the same way.

### 3.3 Agenda Item Types

Each type defines the AI's behavioral approach and the kind of extraction it should perform:

**REFLECT** â€” Look back at something. No decision required.
- AI presents information, invites reaction, moves on.
- Extraction: capture any insights or patterns the user shares.
- Example: reviewing yesterday's wins/misses.

**DECIDE** â€” User needs to make a choice.
- AI presents the decision clearly with 2â€“3 options if applicable. Asks for the user's call.
- One follow-up allowed to clarify if the answer is ambiguous.
- Extraction: capture the decision and its rationale.
- Example: priority triage, project focus selection.

**COMMIT** â€” User is making a promise or setting a deadline.
- AI makes the commitment explicit. Confirms what, who, and when.
- Should ensure the user is intentional, not just casually agreeing.
- Extraction: capture commitment with person, deadline, description.
- Example: "Send Gilbert the pricing page by Thursday."

**INFORM** â€” AI sharing information. No decision required unless user acts on it.
- AI presents concisely, offers to go deeper.
- Extraction: note if user requests follow-up action.
- Example: calendar briefing, schedule overview.

**RESEARCH** â€” AI presenting findings from overnight or Thinking Queue research.
- AI leads with relevance ("This connects to your DRBI work"), then the finding, then recommended action.
- Progressive disclosure â€” headline first, detail only if requested.
- Extraction: capture decision (pursue / skip / research more), may spawn new tasks.
- Example: grant opportunities found overnight.

**PLAN** â€” Time-blocking, scheduling, or sequencing discussion.
- AI proposes a specific plan with times, not just "when do you want to do it?"
- References calendar openings, time estimates, dependencies.
- Extraction: capture time blocks, priority ordering.
- Example: "Block 9â€“10:30 for the curriculum draft."

**FOLLOW-UP** â€” Checking status on something from a previous meeting.
- AI states what was committed and when. Asks for status.
- Direct but not accusatory. Offer help if it's stuck.
- Extraction: update the commitment/waiting-for status.
- Example: "The tablet schema from Dr. Riazati is 8 days overdue."

### 3.4 Agenda Generation Algorithm (Morning Meeting)

The morning agenda is built from these sources, in priority order:

**Critical priority:**
- Overdue blockers (from memories where type = 'blocker', superseded_by IS NULL). Older = higher priority.
- Today's deadlines (commitments or tasks due today).

**High priority:**
- Overdue commitments, weighted by who they're to (commitments to the user's boss or key partners rank higher than self-commitments).
- Calendar events within the next 4 hours that need prep.
- Overnight discoveries with approaching deadlines.
- Habit interventions (streak broken yesterday â€” "never miss twice" trigger).

**Normal priority:**
- Status updates on active projects.
- Overnight research findings.
- Habit check-in (as a single grouped item, not one per habit).
- Thinking Queue results from yesterday.

**Low priority:**
- Life events in the 7â€“30 day window (birthdays, holidays, etc.).
- Stale projects (no activity in 5+ days) â€” "Is this still active?"
- Pattern-based observations from overnight analysis.

**Carryover:**
- Deferred items from previous meetings, with priority escalated.

**Sorting:** Critical first, then high, then normal, then low. Within the same priority level, the AI groups related items (same person, same project) so transitions are natural.

**Budget:** The morning meeting agenda should target 6â€“12 items. If the generator produces more than 12, items below normal priority are held for the next day unless they're time-sensitive. If it produces fewer than 4, the AI keeps the meeting short â€” not every day needs a full agenda.

### 3.5 Agenda Generation (Evening Review)

Compare the morning plan to actuals:
- Which Priority Stack items were completed? â†’ REFLECT items (celebrate wins)
- Which were not? â†’ FOLLOW-UP items (what happened? reschedule?)
- Which habits were completed vs. skipped? â†’ REFLECT item (streak update)
- Any new context from the day the system doesn't know about? â†’ Open-ended ASK item: "Anything happen today I should know about?"
- Light prep for tomorrow? â†’ Optional PLAN item.

### 3.6 Agenda Generation (Weekly Review)

Six phases, each generating its own agenda items:

**Phase 1 â€” CAPTURE:** Single open-ended ASK item. "Before we review, dump everything floating in your head â€” projects, ideas, worries, commitments." The AI extracts each item the user mentions, creates memories/tasks. When the user says "that's it," move to Phase 2.

**Phase 2 â€” REVIEW PROJECTS:** One DECIDE item per active project. "Is this still active? What's the next action?" For stale projects: "This hasn't moved in N days. Still a priority, or shelve it?"

**Phase 3 â€” REVIEW COMMITMENTS & WAITING-FOR:** One FOLLOW-UP item per open commitment and waiting-for. "Still open? Resolved? Need to follow up?"

**Phase 4 â€” HABIT REVIEW:** One REFLECT item. Present the weekly scorecard, identify patterns (which days get skipped, consistency trends), suggest adjustments.

**Phase 5 â€” LOOKING AHEAD:** INFORM items for next week's highlights, approaching deadlines, upcoming events.

**Phase 6 â€” STRATEGIC REFLECTION:** One open-ended ASK item. "Zooming out â€” is your time going where it matters most?" Capture any reprioritization.

### 3.7 Agenda Generation (Strategic Session)

No pre-generated agenda. The AI builds it dynamically:

1. **Turn 1â€“3: UNDERSTAND.** Ask questions to clarify what the user is trying to figure out. Generate agenda items as the scope becomes clear.
2. **Turn 4â€“6: SYNTHESIZE.** Present a framework or set of options. Agenda items emerge as decision points.
3. **Turn 7+: REFINE and COMMIT.** Iterate on the framework, then extract concrete next steps.

Relevant memories are aggressively retrieved before the session starts â€” everything tagged with the topic, related projects, related people.

---

## 4. Conversation Rules

### 4.1 Turn Structure

Every AI turn follows this internal structure:

1. **ACKNOWLEDGE** â€” Respond to what the user just said. Brief, warm, shows the AI actually processed their input. This is NOT optional. The AI never ignores what the user said to jump to the next topic.

2. **ACT** â€” Do something with the information. Extract a task, update a memory, mark an agenda item resolved, note a decision. This may involve tool calls (search memory, check calendar, search email).

3. **BRIDGE** â€” If transitioning to a new topic, connect naturally. "Speaking of timelines..." / "On a different note..." / "While we're talking about Gilbert..." If staying on the same topic for a follow-up, skip the bridge.

4. **PRESENT** â€” Deliver the next agenda item or follow-up question. Include just enough context for the user to respond. End with exactly ONE question or prompt.

### 4.2 The One-Question Rule

**The AI never asks two questions in the same message.** This is the single most important conversational rule. If the AI has follow-up questions, it waits for the user to respond before asking the next one.

The only exception is the opening turn, where the AI gives a brief day snapshot before asking the first question. But even then, the message ends with exactly one question.

### 4.3 Opening Turn Behavior

The first message sets the tone. It does NOT dump a full briefing. Structure:

1. Brief greeting (time-aware, personality-calibrated)
2. One to two sentences of day context (schedule shape, overnight headlines)
3. The single most important thing or the first agenda item
4. One question

**Right:**
"Morning, Chad. Tuesday the 11th. Quiet morning, then Gilbert at 11. The big thing: that curriculum draft is two weeks open. Want to tackle it this morning while you've got a clear block?"

**Wrong:**
"Good morning, Chad! Here's your complete schedule: [10 lines]. Your blockers: [5 lines]. Your commitments: [5 lines]. Your habits: [3 lines]. What would you like to focus on today?"

The briefing information IS available â€” it gets parceled out across the agenda, one item at a time. The user sees the full picture by the end of the meeting, not in the first message.

### 4.4 Follow-Up Behavior

When the user's response is vague or incomplete, the AI probes:

- Vague answer ("Yeah, I'll handle it") â†’ "By when? Want me to block time?"
- Scope ambiguity ("Let's do the grant") â†’ "The Templeton one, or all three?"
- Missing commitment ("That sounds good") â†’ "Should I add that as a task with a deadline?"
- Disconnected decision ("Cancel the meeting") â†’ "Got it. Should I let Gilbert know, or will you?"

**Limits:** Maximum 2 follow-ups per agenda item. If the user gives short answers, take the hint and move on. Never interrogate.

### 4.5 Transition Behavior

Moving between agenda items should feel natural:

**Do:** "Got it, that's tracked. On a related note..." / "Done. Something from last night's research..." / "Good. One more before we wrap up..."

**Never:** "Item 3 of 8: Blocker review" / "Next on the agenda..." / "Moving to agenda item #4..."

The user should never feel like they're in a ticketing system.

### 4.6 Connecting Dots

The AI should make connections the user hasn't explicitly made. This is one of the highest-value behaviors.

**Example:** User is working on a curriculum draft. Overnight research found a grant that requires a pilot program description. The AI connects: "Quick thought â€” the curriculum draft is basically the pilot program description the Templeton grant needs. Two birds, one stone."

**Implementation:** After each user response, the AI should check whether anything the user said or decided connects to other items in memory, other agenda items, or other projects. The prompt should instruct the AI to surface connections when confidence is high.

### 4.7 Respecting Short Answers

If the user gives 3+ consecutive responses under 5 words, the AI should compress:

"Quick version: three things need your call today. [Curriculum draft: block time this morning?] [Gilbert call: briefing ready, lead with grant news.] [Arabic: keep the 12-day streak.] Good with that plan?"

This collapses the remaining agenda into a single confirmation turn. The user can either approve the whole plan or flag a specific item to discuss further.

### 4.8 Closing Behavior

When the agenda queue is empty:

1. **Summary:** Brief recap of outcomes â€” decisions made, tasks added, items deferred. 2â€“4 sentences.
2. **Open floor:** "That's my list. Anything on your mind?"
3. **If user has more:** Generate new agenda items, continue.
4. **If user is done:** Brief send-off, personality-calibrated. "Go crush it. I'll have the Gilbert briefing ready by 10:45." One sentence.

After closing:
- All resolved items â†’ write to memory (decisions, tasks, commitments, waiting-for)
- All deferred items â†’ queued for next meeting with priority + 1
- Conversation summary â†’ stored as conversation-type memory
- Research requests spawned during meeting â†’ queued to Thinking Queue
- If any agenda items require follow-up notifications â†’ scheduled via heartbeat

---

## 5. Extraction Pipeline

Every conversational turn is an opportunity to extract structured data. The extraction happens WITHIN the LLM response, not as a separate call.

### 5.1 What Gets Extracted

**Tasks:** Things the user agreed to do. Includes description, optional deadline, optional project tag, priority.

**Commitments:** Promises made to another person. Includes description, who it's to, deadline.

**Waiting-for:** Things expected from someone else. Includes description, who from, expected date.

**Decisions:** Choices made during the meeting. Includes what was decided, the reasoning/context, and what projects or people it affects.

**Status updates:** New information about a project's state. May supersede an existing memory.

**Agenda modifications:** Items resolved, deferred, or newly created during conversation.

**Research requests:** Things the user or AI identified as needing research. Includes urgency tier (inline / thinking queue / overnight).

### 5.2 Extraction Method

The meeting prompt instructs the AI to append a structured `<meeting_state>` block after each conversational response. This block is parsed by the system and stripped before displaying to the user.

The block contains:
- All extractions from this turn (tasks, commitments, decisions, etc.)
- Agenda updates (which items were resolved, deferred, or added)
- What the next agenda item should be
- Whether this turn had a follow-up or is moving to a new item
- Any async requests the system should execute before the next turn (memory search, calendar check, email search, web search)

This approach means extractions cost zero additional LLM calls â€” they're part of the conversational response.

### 5.3 Extraction Confirmation

When the AI extracts something, it confirms briefly within the conversation:

- Task: "Got it â€” pricing page to Gilbert by Thursday. Tracked."
- Decision: "Alright, Templeton grant is the focus. I'll deprioritize the other two."
- Commitment: "Committed â€” you'll send the draft to the board by Friday."

These confirmations serve two purposes: they let the user correct mistakes ("actually, I said Wednesday"), and they make the extraction visible so the user trusts the system is capturing things.

---

## 6. Async Processing During Meetings

While the user is typing, the system should be working â€” not idle.

### 6.1 Prefetch Strategy

Based on the current agenda item and likely user response, the system executes in parallel:

- **Memory retrieval:** Pre-load memories related to the next 2â€“3 pending agenda items.
- **Person context:** If an upcoming item mentions a person, retrieve their contact graph and recent interactions.
- **Calendar openings:** If a PLAN item is coming up, check what time slots are available.
- **Email search:** If connected, pre-fetch emails related to upcoming FOLLOW-UP items.
- **Drive search:** If connected, find documents related to upcoming items.

These are database queries and API calls, NOT additional LLM calls. The results are injected into the prompt context for the next turn, giving the AI richer information to work with.

### 6.2 Research During Meetings

When the user says "Can you look into that?" or the agenda includes a RESEARCH item:

- **Simple factual lookup** â†’ Web search inline, answer in this turn. ("Let me check... [answer]")
- **Answerable in 1â€“2 searches** â†’ Execute searches, present findings in next turn.
- **Needs substantial research** â†’ Queue to Thinking Queue. Tell user: "I'll research that and have findings within a couple hours."
- **Needs deep analysis** â†’ Queue to overnight batch. Tell user: "I'll do a thorough analysis tonight. You'll see it in tomorrow's briefing."

The AI should communicate which path it's taking so the user knows when to expect results.

### 6.3 Follow-Up Preparation

For each agenda item, the pre-generated `followUps` field contains likely follow-up questions. But the AI should also prepare dynamically based on the user's response patterns:

- If the user tends to give vague answers about deadlines â†’ always have a "by when?" follow-up ready
- If the user tends to agree to too much â†’ have a "is that realistic given your calendar?" check ready
- If the user tends to defer â†’ have a "what would unblock this?" probe ready

---

## 7. Meeting State Management

### 7.1 Meeting States

Each meeting session moves through these states:

**INITIALIZING** â†’ Load pre-generated agenda, prefetch context for first few items, prepare opening message.

**OPENING** â†’ AI delivers the opening turn. One message, no user input yet.

**AGENDA_LOOP** â†’ The main loop. Process user's response, extract data, update agenda, present next item. Repeats until agenda is empty.

**OPEN_FLOOR** â†’ Agenda is empty. AI summarizes outcomes and asks "Anything else?" If the user has more, generate new items and return to AGENDA_LOOP. If not, proceed to CLOSING.

**CLOSING** â†’ AI delivers final summary and send-off. Post-meeting processing fires.

**PAUSED** â†’ User stopped responding mid-meeting (closed app, got distracted). On next open within a reasonable window, resume: "We left off discussing [item]. Want to continue or get the quick summary of what's left?"

**CLOSED** â†’ Meeting is done. All data persisted. Session complete.

### 7.2 State Transition Logic

- INITIALIZING â†’ OPENING: when agenda is loaded and first-turn context is ready.
- OPENING â†’ AGENDA_LOOP: when user sends their first response.
- AGENDA_LOOP â†’ AGENDA_LOOP: when there are still pending agenda items after processing a turn.
- AGENDA_LOOP â†’ OPEN_FLOOR: when all agenda items are resolved, deferred, or skipped.
- OPEN_FLOOR â†’ AGENDA_LOOP: when user introduces new topics (AI generates items).
- OPEN_FLOOR â†’ CLOSING: when user indicates they're done.
- Any state â†’ PAUSED: when user is unresponsive for 5+ minutes.
- PAUSED â†’ AGENDA_LOOP: when user returns and wants to continue.
- PAUSED â†’ CLOSING: when user returns but wants the quick summary instead.

### 7.3 Mid-Meeting Agenda Modification

The agenda is a living queue. During the meeting, items are added, resolved, or deferred:

**User says something that reveals a new topic:**
"Actually, I also need to talk about the hosting migration."
â†’ AI creates a new agenda item, inserts it at appropriate priority, acknowledges: "Added to the list. Want to talk about it now or after we finish [current topic]?"

**User makes a decision that affects other items:**
"Let's cancel the AlmostEnglish launch for now."
â†’ AI resolves the current item AND marks related pending items as skipped (e.g., "Set up landing page" becomes irrelevant).

**User says "skip" / "later" / "not now":**
â†’ Current item moves to deferred. AI acknowledges briefly and moves to next item.

**AI notices an item is no longer relevant:**
During discussion, the AI realizes a pending item was already addressed indirectly by a previous decision.
â†’ AI marks it resolved with a note, doesn't present it, moves on.

---

## 8. Prompt Architecture for Meetings

### 8.1 Prompt Layers

Each meeting turn assembles a prompt from these layers:

**Layer 1 â€” SOUL prompt (stable, cached):** Identity, values, communication style. Same across all interactions. Cached for cost efficiency.

**Layer 2 â€” User context (daily, cached):** Compressed user state â€” active projects, key people, current streaks, schedule shape. Updated once per day by overnight batch. Cached.

**Layer 3 â€” Meeting prompt (per meeting type):** The behavioral rules specific to this meeting type. Includes the conversation rules from Â§4, the extraction instructions from Â§5, the meeting-specific directives from Â§2. Cached per meeting type.

**Layer 4 â€” Turn context (per turn, dynamic):** Current agenda with item statuses. Conversation summary so far (compressed). Prefetched async context (memories, calendar data, email snippets). The specific instruction for this turn based on the state machine state.

**Layer 5 â€” User message (current turn).**

### 8.2 Meeting Prompt Content

The meeting prompt (Layer 3) should include:

1. **Meeting type and rules** â€” the behavioral directives from Â§4 (one question per turn, acknowledge before advancing, etc.)
2. **Item type handling instructions** â€” how to approach each agenda item type from Â§3.3
3. **Extraction format** â€” the `<meeting_state>` block structure from Â§5.2
4. **Available tools** â€” which tools the AI can call (memory search, email, drive, calendar, web search, task creation), dynamically populated based on connected integrations
5. **Personality calibration** â€” the user's detected preferences for tone, detail level, meeting length, accountability style (from personality table)
6. **Current agenda** â€” the full agenda with statuses, priorities, content, pre-loaded questions, and context
7. **Conversation summary** â€” compressed record of what's been discussed so far in this meeting
8. **Prefetched context** â€” results from async pipeline (relevant memories, calendar openings, email snippets for upcoming items)

### 8.3 Context Budget

Meeting prompts need to be token-efficient since they run many turns:

- SOUL prompt: ~2,000 tokens (stable, cached)
- User context: ~800 tokens (daily, cached)
- Meeting prompt + rules: ~1,500 tokens (per type, cacheable)
- Agenda + turn context: ~1,000â€“2,500 tokens (dynamic, changes each turn)
- Conversation summary: grows ~200 tokens per turn, compressed at 5 turns
- User message: varies

**Total per turn:** ~6,000â€“8,000 tokens input. With prompt caching on layers 1â€“3, the incremental cost per turn is primarily the dynamic layer 4 content.

**Conversation summary compression:** After every 5 turns, the rolling conversation summary should be compressed. Replace the full turn-by-turn record with a distilled version: decisions made, items resolved, key facts shared. This prevents the context from growing unboundedly during long weekly reviews.

---

## 9. Adaptive Behavior

### 9.1 Pattern Detection

Over time (5+ meetings), the system should detect and adapt to:

**Meeting length preference:** Does the user engage for 5 minutes or 15? Adjust agenda size accordingly. Short-meeting users get fewer, higher-priority items.

**Response style:** Does the user give detailed responses or terse ones? Adjust follow-up aggressiveness. Terse users get fewer probing questions, more direct suggestions.

**Deferral patterns:** Which item types get deferred most? If the user always defers habit discussions, try presenting them differently (shorter, combined with a task discussion, or moved to evening review).

**Decision speed:** Does the user decide quickly or deliberate? Quick deciders get presented options with a recommendation. Deliberators get more open-ended framing.

**Time-of-day patterns:** Is the user more engaged in morning meetings or evening reviews? Route more important items to the meeting type where the user is most present.

**Accountability response:** Does the user respond better to "the curriculum draft is 14 days overdue" (direct) or "what's blocking the curriculum draft?" (diagnostic)? Calibrate over time.

### 9.2 Where Adaptations Are Stored

Pattern observations are stored as memories with type = 'pattern' and tagged to the personality table. The overnight batch analyzes meeting transcripts and updates these. They're injected into the meeting prompt as personality calibration data (Layer 3).

### 9.3 Handling Repeat Deferrals

If an item is deferred 3+ times, the system should NOT just keep presenting it the same way. Escalation strategy:

- **Deferral 1â€“2:** Normal â€” requeue with priority bump.
- **Deferral 3:** Change approach. Instead of "Status on the curriculum draft?" try "The curriculum draft has come up three times now. What's actually in the way â€” time, clarity, motivation, or something else?"
- **Deferral 4+:** Suggest a structural change. "This keeps getting pushed. Should we break it into smaller pieces? Or is this something to delegate or drop entirely?"
- **Deferral 6+:** Drop from the automatic agenda. Store as a memory note: "User repeatedly deferred [item]. Not surfacing automatically â€” will only mention if user brings it up or a deadline forces it."

---

## 10. Special Conversational Situations

### 10.1 User Goes Off-Script

The user brings up something not on the agenda. The AI should:
1. Go with it. Don't fight for control of the conversation.
2. Capture what the user is sharing (this is valuable context â€” possibly more valuable than the planned agenda).
3. Extract any tasks, decisions, or context updates.
4. When the user finishes, gently return: "Good to know â€” I'll keep that in mind. Back to the plan..."
5. If the off-script topic is substantial, create a new agenda item or queue a Thinking Queue event.

### 10.2 User Asks for a Recommendation

When the user asks "What should I focus on?" or "What's the biggest lever?" â€” the AI gives a SPECIFIC recommendation, not a menu of options.

**Wrong:** "You have tasks from four projects. Which project feels most important today?"
**Right:** "The ThinkDone morning meeting flow test. It's 30 minutes, and it unblocks the seed memory task and the Astro frontend setup. Biggest cascade for the least time."

The recommendation should be justified with one sentence explaining WHY (dependency unblocking, deadline proximity, commitment to someone important, streak maintenance, etc.).

### 10.3 User Reports a Completion

When the user says "that's done" or "I just finished X":
1. Acknowledge with energy calibrated to the significance: "Nice â€” checked off." (routine) vs. "That's a big one. That's been open for two weeks." (milestone)
2. Note downstream effects: "That unblocks the landing page task. Want to bump it up?"
3. Update the scorecard data.
4. If the completion resolves a blocker or commitment, mark those resolved too.

### 10.4 User Wants to Go Deeper

When the user says "Tell me more" or "Let's think about this":
- The AI shifts from meeting-pacing mode to exploration mode for that topic.
- It asks probing questions, draws on memory, and may search email/docs/web.
- When the exploration is complete, it smoothly returns to the agenda.
- The estimated time for the meeting extends â€” the AI should be aware of this but not rush the user.

### 10.5 Interrupted Meeting

User stops responding (closes app, gets pulled away):
- After 5 minutes of no response, meeting enters PAUSED state.
- On next app open within a few hours: "We left off discussing [item]. Want to continue, or should I give you the quick summary of what's left?"
- On next app open much later (2+ hours): Treat remaining items as deferred. Offer fresh context based on what's happened since.

### 10.6 Empty Agenda

Quiet day, overnight batch generated almost nothing:
- Don't force a meeting. "Clean slate today â€” nothing urgent on my end. What are you thinking about working on?"
- If user has nothing: "Enjoy the breathing room. I'm here if anything comes up." End in 2 turns.
- Short meetings are fine. Not every day needs 15 minutes of planning.

### 10.7 Overloaded Agenda

20+ items after a weekend, vacation, or particularly chaotic week:
- Don't present all 20+. Triage on the first turn: "Lot to catch up on. Here are the top 5 that actually need your attention today. The rest I can either handle or hold for tomorrow."
- Present critical and high items only.
- Offer: "There are 12 more items at normal priority. Quick rundown, or should I handle what I can?"

---

## 11. Cross-Meeting Continuity

### 11.1 Carryover Mechanisms

- **Deferred items** â†’ next meeting agenda, priority escalated by one level.
- **Conversation summary** â†’ stored as memory, loaded into next meeting's context.
- **Decisions** â†’ stored as decision-type memories, inform future agenda generation and AI reasoning.
- **Open commitments and waiting-for** â†’ persist until explicitly resolved.
- **Emerging questions** unaddressed during the meeting â†’ carry over as agenda items or get queued to the Thinking Queue.
- **Research requests** spawned during meeting â†’ queued to appropriate tier (Thinking Queue or overnight batch).

### 11.2 Meeting-to-Meeting Learning

After 5+ meetings, the overnight batch should analyze meeting patterns:
- What item types get deferred most often?
- What follow-up questions does the user resist?
- What time of day produces the most productive meetings?
- Which topics generate the most engagement?
- Are meetings getting shorter or longer over time?
- Is the user completing more or fewer Priority Stack items post-meeting?

These analyses feed into the personality calibration (Â§9) and agenda generation tuning.

### 11.3 Evening â†’ Morning Continuity

The evening review captures what the morning meeting couldn't know:
- Which tasks were actually completed.
- What happened in meetings (that the AI didn't attend).
- New context or changes in priorities.
- The user's emotional state and energy level.

This information feeds into the overnight batch, which produces the next morning's agenda with full awareness of what actually happened today â€” not just what was planned.

---

## 12. Success Criteria for the Meeting Engine

The meeting dynamics are working when:

1. **Users engage daily.** The morning meeting is valuable enough to become part of their routine.
2. **Meetings complete in target duration.** 5â€“15 min for morning, not ballooning to 30.
3. **One question per turn is maintained.** No multi-question messages in production.
4. **Extraction accuracy is high.** Tasks, commitments, and decisions match what the user actually said. Users rarely need to correct an extraction.
5. **Connections are surfaced.** The AI makes at least one non-obvious connection per meeting that the user finds valuable.
6. **Deferred items don't pile up.** 80%+ of agenda items are resolved or acted on, not deferred.
7. **Agenda sizing is appropriate.** Users don't feel overwhelmed (too many items) or under-served (too few).
8. **The conversation feels natural.** Users report it feels like talking to a sharp colleague, not filling out a form or talking to a robot.

---

## Appendix A: Example Morning Meeting Transcript

Annotated with agenda state changes and extractions.

```
[STATE: OPENING, Agenda: 6 items pending]

AI: "Morning, Chad. Tuesday the 11th. Quiet morning, then
     Gilbert at 11. The big thing: that curriculum draft is
     two weeks open. Want to tackle it this morning while
     you've got a clear block?"

[Agenda item "curriculum-1" (FOLLOW-UP, high) â†’ ACTIVE]

USER: "Yeah I should. But it's going to take more than a morning.
       I'm stuck on the assessment framework section."

[STATE: AGENDA_LOOP]

AI: "Assessment framework â€” the rubric for measuring student
     progress, right? What specifically has you stuck â€” the
     structure, or figuring out what metrics to use for
     K-12 literacy outcomes?"

[Follow-up 1 of 2 on "curriculum-1"]

USER: "The metrics. I don't know what's standard for faith-based
       literacy programs."

AI: "Good question. I'll research K-12 literacy assessment
     frameworks for faith-based programs â€” expect findings
     in a couple hours. For this morning, draft the other
     sections and leave assessment as a placeholder. That
     unblocks the 80%. Block 9 to 10:30?"

[EXTRACTION: research request (Tier 2) "K-12 literacy assessment
 frameworks faith-based programs"]
[EXTRACTION: task "Curriculum draft 9-10:30, minus assessment section"]
[Agenda item "curriculum-1" â†’ RESOLVED, resolution: "Drafting other
 sections, assessment pending research"]

USER: "Yeah, that works."

AI: "Tracked. Speaking of the Gilbert call at 11 â€” there's a
     development. Overnight research found three grant
     opportunities for DRBI. Strongest is Templeton â€” $50K-200K
     for faith-based ed tech, deadline March 15. Worth
     discussing with Gilbert?"

[Agenda item "discovery-1" (RESEARCH, normal) â†’ ACTIVE]

USER: "Definitely. What's the Templeton grant about?"

AI: "Templeton Foundation, $50K-200K, specifically for innovative
     tech in interfaith understanding. DRBI qualifies. The
     application needs a pilot program description â€” which is
     basically the curriculum draft you're writing this morning.
     One document, double duty. Should I add the grant angle
     to Gilbert's briefing?"

[CONNECTION MADE: curriculum draft â†” Templeton grant application]
[Follow-up on "discovery-1": actionable next step]

USER: "Flag it for Gilbert. He should know."

AI: "Done â€” briefing updated, ready by 10:45. Arabic practice
     is at a 12-day streak. Your stack is coffee â†’ journaling
     â†’ Arabic. Since you're blocking 9 for the draft, do the
     stack before 9 or after the Gilbert call?"

[EXTRACTION: decision "Add Templeton grant to Gilbert briefing"]
[Agenda item "discovery-1" â†’ RESOLVED]
[Agenda item "habits-1" (PLAN, normal) â†’ ACTIVE]

USER: "Before 9. I'll do it now."

AI: "Perfect â€” Arabic by 7:30, curriculum by 9, Gilbert prep
     by 10:45, call at 11. One more: Farsi flashcards got
     skipped yesterday. Never miss twice â€” 15 minutes after
     the Gilbert call?"

[EXTRACTION: plan "Arabic 7:30, curriculum 9:00-10:30, Gilbert prep 10:45"]
[Agenda item "habits-1" â†’ RESOLVED]
[Agenda item "habits-2" (FOLLOW-UP, normal) â†’ ACTIVE]

USER: "Sure, after the call."

[EXTRACTION: task "Farsi flashcards after Gilbert call, 15 min"]
[Agenda item "habits-2" â†’ RESOLVED]

[STATE: OPEN_FLOOR, Agenda: 0 items pending]

AI: "Here's the plan:
     â†’ Now: Coffee â†’ journaling â†’ Arabic (streak: 13 ðŸ”¥)
     â†’ 9:00: Curriculum draft (minus assessment)
     â†’ 10:45: Gilbert prep (grant news included)
     â†’ 11:00: Gilbert call
     â†’ After call: Farsi flashcards

     That's my list. Anything on your mind?"

USER: "Nope, let's go."

[STATE: CLOSING]

AI: "Go get it. Research and briefing will be ready when you
     need them."

[STATE: CLOSED]
[POST-MEETING: 3 tasks created, 1 research request queued,
 1 decision stored, 1 connection identified, 0 items deferred]
```